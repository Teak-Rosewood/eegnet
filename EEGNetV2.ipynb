{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Running installs\n",
        "!pip install tensorflow==2.8.0\n",
        "!pip install keras==2.8.0"
      ],
      "metadata": {
        "id": "Tog9xoZLqyDn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "UXba2xh0G3Nb"
      },
      "outputs": [],
      "source": [
        "# Importing Libraries\n",
        "import os\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TPU Initialization\n",
        "try:\n",
        "  tpu_address = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  tpu = tf.distribute.cluster_resolver.TPUClusterResolver(tpu_address)\n",
        "  tf.config.experimental_connect_to_cluster(tpu)\n",
        "  tf.tpu.experimental.initialize_tpu_system(tpu)\n",
        "  strategy = tf.distribute.TPUStrategy(tpu)\n",
        "  print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
        "  print(\"Number of accelerators: \", strategy.num_replicas_in_sync)\n",
        "except ValueError:\n",
        "  print('TPU failed to initialize.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p6K2iroFrOW7",
        "outputId": "069f2314-79e0-4c72-9626-5127a2753eee"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running on TPU  ['10.61.151.146:8470']\n",
            "Number of accelerators:  8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment Variables\n",
        "BATCH_SIZE = 128\n",
        "REPLICAS = strategy.num_replicas_in_sync"
      ],
      "metadata": {
        "id": "zgHg7cB7z91G"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "yyNb2iy8HBQr"
      },
      "outputs": [],
      "source": [
        "class EEGNetV2(tf.keras.Model):\n",
        "  def __init__(self, classes):\n",
        "    super(EEGNetV2, self).__init__()\n",
        "\n",
        "    self.classes = classes\n",
        "    self.channels = 64\n",
        "    self.samples = 128\n",
        "    self.kernel_length = 128\n",
        "    self.pool_length = 8\n",
        "    self.num_filters = 8\n",
        "\n",
        "    self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
        "\n",
        "    self.conv1 = tf.keras.layers.Conv2D(8, (1, self.kernel_length), padding = 'same', use_bias = False)\n",
        "    self.bn1 = tf.keras.layers.BatchNormalization(axis = 1)\n",
        "    self.DWConv1 = tf.keras.layers.DepthwiseConv2D((self.channels, 1), depth_multiplier = 2, depthwise_constraint = tf.keras.constraints.max_norm(1.), use_bias = False)\n",
        "    self.bn2 = tf.keras.layers.BatchNormalization(axis = 1)\n",
        "    self.AvPool1 = tf.keras.layers.AveragePooling2D((1, self.pool_length))\n",
        "    self.SepConv1 = tf.keras.layers.SeparableConv2D(16, (1,16), padding='same', use_bias = False)\n",
        "    self.bn3 = tf.keras.layers.BatchNormalization(axis = 1)\n",
        "    self.AvPool2 = tf.keras.layers.AveragePooling2D((1, 8))\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    self.classifier = tf.keras.layers.Dense(self.classes, activation = 'softmax')\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.conv1(inputs)\n",
        "    x = self.bn1(x)\n",
        "    x = self.DWConv1(x)\n",
        "    x = self.bn2(x)\n",
        "    x = tf.keras.activations.elu(x)\n",
        "    x = self.AvPool1(x)\n",
        "    x = self.dropout1(x)\n",
        "    x = self.SepConv1(x)\n",
        "    x = self.bn3(x)\n",
        "    x = tf.keras.activations.elu(x)\n",
        "    x = self.AvPool2(x)\n",
        "    x = self.flatten(x)\n",
        "    return self.classifier(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "hQa_lurAQpX3"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  # Initialising the model\n",
        "  model = EEGNetV2(4)\n",
        "\n",
        "  # Optimizer, Accuracy & Loss Function\n",
        "  optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "  test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "  loss_object = tf.keras.losses.CategoricalCrossentropy(reduction = tf.keras.losses.Reduction.NONE)\n",
        "  train_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'train_accuracy')\n",
        "  test_accuracy = tf.keras.metrics.CategoricalAccuracy(name = 'test_accuracy')\n",
        "\n",
        "  def compute_loss(y, predictions):\n",
        "    per_example_loss = loss_object(y, predictions)\n",
        "    return tf.nn.compute_average_loss(per_example_loss, global_batch_size= BATCH_SIZE * REPLICAS)\n",
        "\n",
        "  def train_step(inputs):\n",
        "    x, y = inputs\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "      predictions = model(x)\n",
        "      loss = compute_loss(y, predictions)\n",
        "\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    train_accuracy.update_state(y, predictions)\n",
        "\n",
        "    return loss\n",
        "\n",
        "  def test_step(inputs):\n",
        "    x, y = inputs\n",
        "\n",
        "    predictions = model(x)\n",
        "    loss = loss_object(y, predictions)\n",
        "\n",
        "    test_loss.update_state(loss)\n",
        "    test_accuracy.update_state(y, predictions)\n",
        "\n",
        "  @tf.function\n",
        "  def distributed_train_step(dataset_inputs):\n",
        "    per_replica_losses = strategy.run(train_step,args=(dataset_inputs,))\n",
        "    return strategy.reduce(tf.distribute.ReduceOp.SUM, per_replica_losses,\n",
        "                           axis=None)\n",
        "\n",
        "  @tf.function\n",
        "  def distributed_test_step(dataset_inputs):\n",
        "    strategy.run(test_step, args=(dataset_inputs,))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preperation (make sure data is of tf.data.Dataset format)\n",
        "x_train  = tf.random.uniform(shape=[1000, 64, 480, 1])\n",
        "y_train = tf.zeros(shape=[1000, 4])\n",
        "x_test  = tf.random.uniform(shape=[128, 64, 480, 1])\n",
        "y_test = tf.zeros(shape=[128, 4])\n",
        "AUTO = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "def get_batched_dataset(x, y):\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((x, y))\n",
        "  ataset = dataset.shuffle(2048)\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  dataset = dataset.prefetch(AUTO)\n",
        "  return dataset\n",
        "\n",
        "def get_train_dataset():\n",
        "  dataset = get_batched_dataset(x_train, y_train)\n",
        "  dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "  return dataset\n",
        "\n",
        "def get_test_dataset():\n",
        "  dataset = get_batched_dataset(x_test, y_test)\n",
        "  dataset = strategy.experimental_distribute_dataset(dataset)\n",
        "  return dataset"
      ],
      "metadata": {
        "id": "BBMwLwtp3j_v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 1\n",
        "with strategy.scope():\n",
        "  for epoch in range(EPOCHS):\n",
        "    total_loss = 0.0\n",
        "    num_batches = 0\n",
        "\n",
        "    for x in get_train_dataset():\n",
        "      total_loss += distributed_train_step(x)\n",
        "      num_batches += 1\n",
        "    train_loss = total_loss / num_batches\n",
        "\n",
        "    for x in get_test_dataset():\n",
        "      distributed_test_step(x)\n",
        "\n",
        "    template = (\"Epoch {}, Loss: {:.2f}, Accuracy: {:.2f}, Test Loss: {:.2f}, \"\n",
        "                \"Test Accuracy: {:.2f}\")\n",
        "    tf.print (template.format(epoch+1, train_loss,\n",
        "                           train_accuracy.result()*100, test_loss.result() / strategy.num_replicas_in_sync,\n",
        "                           test_accuracy.result()*100))\n",
        "\n",
        "    test_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "    test_accuracy.reset_states()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "3gD_qKZr2Fa2",
        "outputId": "648ab4ef-12cb-49ee-fce3-a48349c20d56"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-39d87479dd3a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# TESTING LOOP\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mget_test_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m       \u001b[0mdistributed_test_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3563f4bbb3c0>\u001b[0m in \u001b[0;36mget_test_dataset\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_test_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_batched_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstrategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_distribute_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-3563f4bbb3c0>\u001b[0m in \u001b[0;36mget_batched_dataset\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_batched_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0mataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_remainder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    791\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    792\u001b[0m     \"\"\"\n\u001b[0;32m--> 793\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    794\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    795\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element, is_files, name)\u001b[0m\n\u001b[1;32m   4489\u001b[0m         tensor_shape.dimension_value(self._tensors[0].get_shape()[0]))\n\u001b[1;32m   4490\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4491\u001b[0;31m       batch_dim.assert_is_compatible_with(\n\u001b[0m\u001b[1;32m   4492\u001b[0m           tensor_shape.Dimension(\n\u001b[1;32m   4493\u001b[0m               tensor_shape.dimension_value(t.get_shape()[0])))\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36massert_is_compatible_with\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    292\u001b[0m     \"\"\"\n\u001b[1;32m    293\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m       raise ValueError(\"Dimensions %s and %s are not compatible\" %\n\u001b[0m\u001b[1;32m    295\u001b[0m                        (self, other))\n\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Dimensions 1000 and 128 are not compatible"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vWUKlLARFcQg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}